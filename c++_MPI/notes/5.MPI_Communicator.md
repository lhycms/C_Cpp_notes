# MPI `Communicator`

# `MPI` 进程组 (group) 通信
<font color="pink" size="4">

https://mpitutorial.com/tutorials/introduction-to-groups-and-communicators/

概念
---
- `group`: A `group` is an ordered set of process identifiers. The ordering is given by associating with each process identifier a unique `rank` from 0 to `group.size - 1`
- `communicator`: 是一组可以相互通信的进程，可以由来自单个组或多个组的进程组成。


Note
----
1. 通信器(`communicator`)包含组(`group`)，而组(`group`)只是进程(`process`)的集合。

2. 如果一个通信器只包含一个组，则称为`内部通信器`（通信器 `MPI_COMM_WORLD` 就是这样一种，它指的是一个包含所有 `rank` 的 `group`）。

3. 如果它包含两个组，则在 `intercommunicator` 中调用它（默认情况下没有为您创建）。 

</font>


# 前言
<font color="pink" size="4">

- In all previuous tutorials, we have used the communicator `MPI_COMM_WORLD`. For simple applications, this is sufficient as we have a `relatively small number of processes` and we usually either want to talk to `one of them or all of them at one time`. 
- When applications start to `get bigger`, this becomes less practical and we may only want to talk to a few processes at once. In this lesson, we show how to create new communicators to communicate with `a subset of the original group of processes` at once.


</font>


# 1. Overview of communicator
<font color="pink" size="4">

- As we have seen when learning about `collective routines`, MPI allows you to talk to all processes in a communicator at once to do things like distribute data from one process to many processes using `MPI_Scatter` or perform a data reduction using `MPI_Reduce`. However, up to now, we have only used the default communicator, `MPI_COMM_WORLD`.
- For simple applications, it’s not unusual to do everything using `MPI_COMM_WORLD`, but for more complex use cases, it might be helpful to have `more communicators`. An example might be if you wanted to perform calculations on a subset of the processes in a grid. For instance, all processes in each row might want to sum a value. This brings us to the first and most common function used to create new communicators:
```c++
MPI_Comm_split(
        MPI_Comm comm,
        int color,
        int key,
        MPI_Comm *newcomm)
```

</font>

## 1.1. `MPI_Comm_split()`: 创造新的 `sub communicators`

<font color="orange" size="4">

Note
----
1. It’s important to note here that the original communicator doesn’t go away. but a new communicator is created on each process. 
2. creates new communicators by splitting a communicator into a group of `sub-communicators` based on the input values `color` and `key`.

</font>

```c++
MPI_Comm_split(
        MPI_Comm comm,
        int color,
        int key,
        MPI_Comm *newcomm)

/* 
Parameters
----------
    1. comm: the communicator that will be used as the basis for the new communicators. This could be `MPI_COMM_WORLD`, but it could be any other communicator as well.
    2. color: 第二个参数 color 决定了每个进程将属于哪个新的communicator。传递相同颜色值的所有进程都分配给相同的通信器。如果颜色是 `MPI_UNDEFINED`，则该进程将不会包含在任何新的通信器中。
    3. key: 决定了每个新通信器中的排序（rank）。 传入键最小值的过程将是 rank=0 ，下一个最小的将是 rank=1，依此类推。 如果两个进程的key相等，则原始 communicator 中 lower rank 的进程将首先出现。
    4. newcomm: is how MPI returns the new communicator back to the user.
*/
```

## 1.2. Example of using multiple communicators
<font color="orange" size="4">

- Now let’s look at a simple example where we attempt to `split a single global communicator into a set of smaller communicators`. In this example, we’ll imagine that we’ve logically `laid out our original communicator into a 4x4 grid of 16 processes` and we want to divide the grid by row. To do this, each row will get its own color. In the image below, you can see how each group of processes with the same color on the left ends up in its own communicator on the right.

</font>

```c++
#include <iostream>
#include "mpi.h"


int main(int argc, char **argv) {
    // Initialize the MPI
    MPI_Init(&argc, &argv);
    int world_rank, world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    int color = world_rank / 4; // Determine color based on row

    // Split the communicator based on the color and use the 
    // original rank for ordering
    MPI_Comm row_comm;
    MPI_Comm_split(MPI_COMM_WORLD, color, world_rank, &row_comm);

    int row_rank, row_size;
    MPI_Comm_size(row_comm, &row_size);
    MPI_Comm_rank(row_comm, &row_rank);

    printf("WORLD RANK/SIZE: %d/%d \t ROW RANK/SIZE: %d/%d\n",
	            world_rank, world_size, row_rank, row_size);

    MPI_Comm_free(&row_comm);

    MPI_Finalize();
    return 0;
}
```
Output:
```sh
$ mpicxx main.cpp -o main
$ mpiexec -n 16 ./main
WORLD RANK/SIZE: 0/16    ROW RANK/SIZE: 0/4
WORLD RANK/SIZE: 1/16    ROW RANK/SIZE: 1/4
WORLD RANK/SIZE: 2/16    ROW RANK/SIZE: 2/4
WORLD RANK/SIZE: 3/16    ROW RANK/SIZE: 3/4
WORLD RANK/SIZE: 4/16    ROW RANK/SIZE: 0/4
WORLD RANK/SIZE: 5/16    ROW RANK/SIZE: 1/4
WORLD RANK/SIZE: 6/16    ROW RANK/SIZE: 2/4
WORLD RANK/SIZE: 7/16    ROW RANK/SIZE: 3/4
WORLD RANK/SIZE: 8/16    ROW RANK/SIZE: 0/4
WORLD RANK/SIZE: 9/16    ROW RANK/SIZE: 1/4
WORLD RANK/SIZE: 10/16   ROW RANK/SIZE: 2/4
WORLD RANK/SIZE: 11/16   ROW RANK/SIZE: 3/4
WORLD RANK/SIZE: 12/16   ROW RANK/SIZE: 0/4
WORLD RANK/SIZE: 13/16   ROW RANK/SIZE: 1/4
WORLD RANK/SIZE: 14/16   ROW RANK/SIZE: 2/4
WORLD RANK/SIZE: 15/16   ROW RANK/SIZE: 3/4
```

<font color="red" size="4">

Note
----
1. 最后，我们使用 `MPI_Comm_free()` 释放 `communicator`。 这似乎不是一个重要的步骤，但它与在任何其他程序中完成它时释放内存一样重要。 当一个 MPI 对象不再被使用时，它应该被释放以便以后可以重用。 `MPI 一次可以创建的对象数量有限，如果 MPI 用完可分配的对象，不释放对象可能会导致运行时错误。`

</font>


# 2. Other communicator creation functions
<font color="orange" size="4">

1. While `MPI_Comm_split()` is the most common communicator creation function, there are many others. 
2. `MPI_Comm_dup` is the most basic and creates a `duplicate` of a communicator. 存在一个仅创建副本的函数可能看起来很奇怪，但这对于使用库来执行专门功能的应用程序非常有用，例如数学库。 在这些类型的应用程序中，重要的是用户代码和库代码不会相互干扰。 为了避免这种情况，每个应用程序应该做的第一件事是创建 `MPI_COMM_WORLD` 的副本，这将避免其他库也使用 `MPI_COMM_WORLD` 的问题。 库本身也应该复制 `MPI_COMM_WORLD` 以避免同样的问题
3. `MPI_Comm_create`: At first glance, this function looks very similar to `MPI_Comm_create_group`. Its signature is almost identical:
```c++
MPI_Comm_create(
	MPI_Comm comm,
	MPI_Group group,
    MPI_Comm* newcomm)

/*
Note
----
The key difference however (besides the lack of the tag argument), is that MPI_Comm_create_group is only collective over the group of processes contained in group, where MPI_Comm_create is collective over every process in comm. This is an important distinction as the size of communicators grows very large. If trying to create a subset of MPI_COMM_WORLD when running with 1,000,000 processes, it’s important to perform the operation with as few processes as possible as the collective becomes very expensive at large sizes.
*/
```

</font>


# 3. Difference between `inter-communicators` and `intra-communicators`