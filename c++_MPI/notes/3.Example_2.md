# 1. Example 2
## 1.1. `MPI_Wtime()`
```c++
double start_time = MPI_Wtime();

...

double end_time = MPI_Wtime();

double last_time = end_time - start_time;
```

## 1.2. Code
<font color="pink" size="4">

- Let’s learn something about which parts of this code account for most of the run time. MPI provides a timer, `MPI_Wtime()`, which returns the `current walltime`. We can use this function to determine how long each section of the code takes to run.
- As the above code indicates, we don’t really want every rank to print the timings, since that could look messy in the output.

</font>

```c++
/* Filename: main.cpp */
#include <iostream>
#include "mpi.h"

int main(int argc, char **argv) {
    // Initialize MPI
    int world_size, my_rank;
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);

    int N = 200000000;

    // Initialize a 
    double start_time = MPI_Wtime();
    double *a = new double[N];
    for (int i=0; i<N; i++) {
        a[i] = 1.0;
    }

    double end_time = MPI_Wtime();
    if (my_rank == 0 ) {
        std::cout << "Initialize a time: " << end_time - start_time << std::endl;
    }

    // Initialize b
    start_time = MPI_Wtime();
    double *b = new double[N];
    for (int i=0; i<N; i++) {
        b[i] = 1.0 + double(i);
    }
    end_time = MPI_Wtime();
    if (my_rank == 0 ) {
        std::cout << "Initialize b time: " << end_time - start_time << std::endl;
    }

    // Add the two arrays
    start_time = MPI_Wtime();
    for (int i=0; i<N; i++) {
        a[i] = a[i] + b[i];
    }
    end_time = MPI_Wtime();
    if (my_rank == 0 ) {
        std::cout << "Add arrays time: " << end_time - start_time << std::endl;
    }

    // Average the result
    start_time = MPI_Wtime();
    double average = 0.0;
    for (int i=0; i<N; i++) {
        average += a[i] / double(N);
    }
    end_time = MPI_Wtime();
    if (my_rank == 0 ) {
        std::cout << "Average result time: " << end_time - start_time << std::endl;
    }

    std::cout.precision(12);
    if (my_rank == 0 ) {
        std::cout << "Average: " << average << std::endl;
    }
    delete [] a;
    delete [] b;

    MPI_Finalize();
    
    return 0;
}
```

Output:
```sh
$ mpicxx main.cpp -o main
$ mpirun -n 2 ./main
Initialize a time: 1.07485
Initialize b time: 1.00666
Add arrays time: 0.805052
Average result time: 1.38827
Average: 100000001.5
```

# 2. Point-to-Point Communication
<font color="gree" size="4">

上述程序的缺陷：
------------
1. Running on multiple ranks doesn't help with timings, because each rank is duplicating all of the same work. 
2. In some ways, running on multiple ranks makes the timing worse, because all of the processes are forced to complete for the same resource.
3. Memory bandwidth in particular is likely a serious problem due to the extreme large arrays that must be accessed and manipulated by each process.
4. `We want the ranks cooperate on the problem`, with each rank working on different part of calculation.
5. In this example, that means that different ranks will work on different parts of arrays `a` and `b`, and then the results on each rank will be summed across all the ranks.

</font>

## 2.1. 进程间的 `point-to-point communication`
<font color="orange" size="4">

1. In this section, we will handle the details of communication between `processes` using `point-to-point` communication.
2. `Point-to-point communication` involves cases in which a code instructs one specific process to `send/receive` information `to/from` anothor specific process. 
3. The primary functions associated with this approach are:
   1. `MPI_Send()`
   2. `MPI_Recv()`

```c++
int MPI_Send(const void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm);
/*
Parameters
----------
    1. buf: pointer to the start of the buffer being sent.
    2. count: number of elements to send.
    3. datatype: MPI data type of each element
    4. dest: rank of destination process
    5. tag: message tag
    6. comm: MPI communicator
*/


int MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status);
/*
Parameters
----------
    1. buf: pointer to the start of the buffer to receive the message.
    2. count: maximum number of elements the buffer can hold.
    3. datatype: MPI Datatype of each elements
    4. source: the rank of source process -- `MPI_ANY_SOURCE` matches any process
    5. tag: message tag ( integer>=0 ), `MPI_ANY_TAG` matches any tag
    6. comm: MPI communicator
    7. status: pointer to the structure in which to store status
*/
```

</font>

## 2.2. `Rank's worldload`

<font color="orange" size="4">

We need to decide what parts of the arrays each of the ranks will work on; this is more generally known as a `rank’s workload`. Add the following code just before the initialization of array `a`:
```c++
// Determine the workload of each rank
int workloads[world_size];
for (int i=0; i<world_size; i++) {
    workloads[i] = N / world_size;
    if ( i < N % world_size )
        workloads[i]++;
}

int my_start = 0;

for (int i=0; i<my_rank;i++) {
    my_start += workloads[i];
}

int my_end = my_start + workloads[my_rank];
```

In above code, `my_start` and `my_end` represent the range over which each rank will perform mathematical opeartions on the arrays.

</font>

```c++
/* Filename: main.cpp */
#include <iostream>
#include "mpi.h"

int main(int argc, char **argv) {
    // Initialize MPI
    int world_size, my_rank;
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);

    int N = 200000000;

    int workloads[world_size];
    for (int i=0; i<world_size; i++) {
        workloads[i] = N / world_size;
        if ( i < N % world_size ) 
            workloads[i]++;
    }

    int my_start = 0; 
    for (int i=0; i<my_rank; i++) {
        my_start += workloads[i];
    }
    int my_end = my_start + workloads[my_rank];


    // Initialize a 
    double start_time = MPI_Wtime();
    double *a = new double[N];
    for (int i=my_start; i<my_end; i++) {
        a[i] = 1.0;
    }

    double end_time = MPI_Wtime();
    if (my_rank == 0 ) {
        std::cout << "Initialize a time: " << end_time - start_time << std::endl;
    }

    // Initialize b
    start_time = MPI_Wtime();
    double *b = new double[N];
    for (int i=my_start; i<my_end; i++) {
        b[i] = 1.0 + double(i);
    }
    end_time = MPI_Wtime();
    if (my_rank == 0 ) {
        std::cout << "Initialize b time: " << end_time - start_time << std::endl;
    }

    // Add the two arrays
    start_time = MPI_Wtime();
    for (int i=my_start; i<my_end; i++) {
        a[i] = a[i] + b[i];
    }
    end_time = MPI_Wtime();
    if (my_rank == 0 ) {
        std::cout << "Add arrays time: " << end_time - start_time << std::endl;
    }

    // Average the result
    start_time = MPI_Wtime();
    double average = 0.0;
    for (int i=my_start; i<my_end; i++) {
        average += a[i] / double(N);
    }
    end_time = MPI_Wtime();
    if (my_rank == 0 ) {
        std::cout << "Average result time: " << end_time - start_time << std::endl;
    }

    if (my_rank == 0) {
        for (int i=1; i<world_size; i++) {
            double partial_average;
            MPI_Status status;
            MPI_Recv(&partial_average, 1, MPI_DOUBLE, i, 77, MPI_COMM_WORLD, &status);
            average += partial_average;
        }
    }
    else {
        MPI_Send(&average, 1, MPI_DOUBLE, 0, 77, MPI_COMM_WORLD);
    }


    std::cout.precision(12);
    if (my_rank == 0 ) {
        std::cout << "Average: " << average << std::endl;
    }
    delete [] a;
    delete [] b;

    MPI_Finalize();
    
    return 0;
}
```

Output:
```sh
$ mpicxx main.cpp -o main
$ mpiexec -n 8 ./main
Initialize a time: 0.166666
Initialize b time: 0.151037
Add arrays time: 0.109116
Average result time: 0.209875
Average: 100000001.5
```

# 3. MPI data type
<font color="orange" size="4">

1. `MPI_BYTE`: 8 binary digits
2. `MPI_CHAR`: char
3. `MPI_UNSIGNED_CHAR`: unsigned char
4. `MPI_SHORT`: signed short int
5. `MPI_UNSIGNED_SHORT`: unsigned short int
6. `MPI_INT`: signed int
7. `MPI_UNSIGNED`: unsigned int
8. `MPI_LONG`: signed long int
9. `MPI_UNSIGNED_LONG`: unsigned long int
10. `MPI_FLOAT`: float
11. `MPI_DOUBLE`: double
12. `MPI_PACKED`: <font color="gree">define your own with `MPI_Pack / MPI_Unpack`</font>

</font>


# 4. Reducing the Memory Footprint
<font color="orange" size="4">

- Now, `time` is only one resource we concern about. Another resource is often that is even more important is `memory`.
- Running on multiple processes might decrease our running time, but it increases our memory footprint.

</font>

## 4.1. 修改上述程序，降低 `Memory Footprint`
<font color="orange" size="4">

- Of course, there isn’t really a good reason for each rank to allocate the entire arrays of size N, because each rank will only ever use values within the range of `my_start` to `my_end`. Let’s modify the code so that each rank allocates a and b to a size of `workloads[my_rank]`.   

</font>

```c++
// Replace the initialization of a with:
double *a = new double[ workloads[my_rank] ];
for (int i=0; i<workloads[my_rank]; i++) {
    a[i] = 1.0;
}

// Replace the range of the loops that add and average the arrays to for (int i=0; i<workloads[my_rank]; i++).
// ...
```

## 4.2. 修改后的完整代码
```c++
#include <iostream>
#include "mpi.h"


int main(int argc, char **argv) {
    // Initialize a MPI
    MPI_Init(&argc, &argv);
    int world_size, my_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);

    int N = 200000000;

    // Allocate memory footprint to specific rank.
    int workloads[world_size];
    for (int i=0; i<world_size; i++) {
        workloads[i] = N / world_size;
        if ( i < N%world_size )
            workloads[i] += 1;
    }
    
    int my_start = 0;
    for (int i=0; i<my_rank; i++) {
        my_start += workloads[i];
    }
    int my_end = my_start + workloads[my_rank];

    // Initialize a 
    double start_time = MPI_Wtime();
    int *a = new int[ workloads[my_rank] ];
    for (int i=0; i<workloads[my_rank]; i++) {
        a[i] = 1.0;
    }
    double end_time = MPI_Wtime();
    if (my_rank == 0) 
        std::cout << "Time: " << end_time - start_time << std::endl;

    // Initialize b
    start_time = MPI_Wtime();
    int *b = new int[ workloads[my_rank] ];
    for (int i=0; i<workloads[my_rank]; i++) {
        b[i] = 1.0 + double(i + my_start);
    }
    end_time = MPI_Wtime();
    if (my_rank == 0) 
        std::cout << "Time: " << end_time - start_time << std::endl;
    
    // Add the two arrays
    start_time = MPI_Wtime();
    for (int i=0; i<workloads[my_rank]; i++) {
        a[i] = a[i] + b[i];
    }
    end_time = MPI_Wtime();
    if (my_rank == 0) 
        std::cout << "Time: " << end_time - start_time << std::endl;

    // Average the result
    start_time = MPI_Wtime();
    double average = 0;
    for (int i=0; i<workloads[my_rank]; i++) {
        average += double(a[i]) / N;
    }
    end_time = MPI_Wtime();
    if (my_rank == 0) 
        std::cout << "Time: " << end_time - start_time << std::endl;

    if (my_rank == 0) {
        for (int i=1; i<world_size; i++) {
            double partial_average;
            MPI_Status status;
            MPI_Recv(a, workloads[my_rank], MPI_INT, i, 77, MPI_COMM_WORLD, &status);
            MPI_Recv(b, workloads[my_rank], MPI_INT, i, 77, MPI_COMM_WORLD, &status);
            MPI_Recv(&partial_average, 1, MPI_DOUBLE, i, 77, MPI_COMM_WORLD, &status);
            average += partial_average;
        }  
    }
    else {
        MPI_Send(a, workloads[my_rank], MPI_INT, 0, 77, MPI_COMM_WORLD);
        MPI_Send(b, workloads[my_rank], MPI_INT, 0, 77, MPI_COMM_WORLD);
        MPI_Send(&average, 1, MPI_DOUBLE, 0, 77, MPI_COMM_WORLD);
    }

    std::cout.precision(12);
    if (my_rank == 0 ) {
        std::cout << "Average: " << average << std::endl;
    }
    delete [] a;
    delete [] b;   

    MPI_Finalize();

    return 0;
}
```
Output:
```sh
Time: 0.135269
Time: 0.118507
Time: 0.0931066
Time: 0.210038
Average: 100000001.5
```


# 5. Collective Communication
<font color="gree" size="4">

Previously, we used `point-to-point communication` (i.e. `MPI_Send` and `MPI_Recv`) to sum the results across all ranks.

</font>

```c++
// Point-to-point communication
    if ( my_rank == 0 ) {
    for (int i=1; i<world_size; i++) {
        double partial_average;
        MPI_Status status;
        MPI_Recv( &partial_average, 1, MPI_DOUBLE, i, 77, MPI_COMM_WORLD, &status );
        average += partial_average;
    }
    }
    else {
    MPI_Send( &average, 1, MPI_DOUBLE, 0, 77, MPI_COMM_WORLD );
    }
```

<font color="gree" size="4">

`Collective Communication`
-------------------------
`MPI` 提供了许多集体通信功能，这些功能可以`自动执行(简化)许多仅使用点对点通信编写起来可能很复杂的过`。 一种特别有用的集体通信函数是 `MPI_Reduce()`：
```c++
int MPI_Reduce(const void *setbuf, void *recvbuf, int count, MPI_Datatype dataytyp, MPI_Op op, int root, MPI_Comm comm);

/*
Parameters
----------
    1. sendbuf: address of send buffer
    2. recvbuf: address of receive buffer
    3. count: number of elements in send buffer
    4. datatype: MPI data type of each element
    5. op: reduce opeartion
    6. root: rank of root process
    7. comm: the communicator to use
*/
```

</font>

## 5.1. Possible values for `op` are
<font color="pink" size="4">

Parameters for `op` in `MPI_Reduce()` function
----------
1. `MPI_MAX`: maximum -- `integer, float`
2. `MPI_MIN`: minimum -- `integer, float`
3. `MPI_SUM`: sum -- `integer, float`
4. `MPI_PROD`: product -- `integer, float`
5. `MPI_LAND`: logical AND -- `integer`
6. `MPI_BAND`: bit-wise AND -- `integer,MPI_BYTE`
7. `MPI_LOR`: logical OR -- `integer`
8. `MPI_BOR`: bit-wise OR -- `integer,MPI_BYTE`
9. `MPI_LXOR`: logical XOR -- `integer`
10. `MPI_BXOR`: bit-wise XOR -- `integer,MPI_BYTE`
11. `MPI_MAXLOC`: max value and location -- `	float`
12. `MPI_MINLOC`: min value and location -- `float`


We will use the `MPI_Reduce()` function to sum a value across all ranks, `without all of the point-to-point communication code we needed earlier`. Replace all of your point-to-point communication code above with:
```c++
double partial_average = average;
MPI_Reduce(&partial_average, &average, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
```

</font>

<font color="red" size="4">

Note
----
1. 请注意，除了使我们能够编写看起来`更简单`的代码之外，集体通信操作往往比我们尝试使用点对点调用编写自己的通信操作来实现的`更快`。

</font>

## 5.2. 修改后的完整代码
```c++
#include <iostream>
#include "mpi.h"


int main(int argc, char **argv) {
    // Initialize a MPI
    MPI_Init(&argc, &argv);
    int world_size, my_rank;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);

    int N = 200000000;

    // Allocate memory footprint to specific rank.
    int workloads[world_size];
    for (int i=0; i<world_size; i++) {
        workloads[i] = N / world_size;
        if ( i < N%world_size )
            workloads[i] += 1;
    }
    
    int my_start = 0;
    for (int i=0; i<my_rank; i++) {
        my_start += workloads[i];
    }
    int my_end = my_start + workloads[my_rank];

    // Initialize a 
    double start_time = MPI_Wtime();
    int *a = new int[ workloads[my_rank] ];
    for (int i=0; i<workloads[my_rank]; i++) {
        a[i] = 1.0;
    }
    double end_time = MPI_Wtime();
    if (my_rank == 0) 
        std::cout << "Time: " << end_time - start_time << std::endl;

    // Initialize b
    start_time = MPI_Wtime();
    int *b = new int[ workloads[my_rank] ];
    for (int i=0; i<workloads[my_rank]; i++) {
        b[i] = 1.0 + double(i + my_start);
    }
    end_time = MPI_Wtime();
    if (my_rank == 0) 
        std::cout << "Time: " << end_time - start_time << std::endl;
    
    // Add the two arrays
    start_time = MPI_Wtime();
    for (int i=0; i<workloads[my_rank]; i++) {
        a[i] = a[i] + b[i];
    }
    end_time = MPI_Wtime();
    if (my_rank == 0) 
        std::cout << "Time: " << end_time - start_time << std::endl;

    // Average the result
    start_time = MPI_Wtime();
    double average = 0;
    for (int i=0; i<workloads[my_rank]; i++) {
        average += double(a[i]) / N;
    }
    end_time = MPI_Wtime();
    if (my_rank == 0) 
        std::cout << "Time: " << end_time - start_time << std::endl;

    // Collective Communication
    double partial_average = average;
    MPI_Reduce(&partial_average, &average, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

    std::cout.precision(12);
    if (my_rank == 0 ) {
        std::cout << "Average: " << average << std::endl;
    }
    delete [] a;
    delete [] b;   

    MPI_Finalize();

    return 0;
}
```