# Content
<font color="orange" size=4>

MPI ( Message passing interface )
---------------------------------
1. MPI is a message passing standard which is designed to work on a variety of `parallel computing archtecture`. 

Questions
---------
1. How can I use MPI to parallelize a compiled code?

Objectives
----------
1. Compile and run C++ codes that are parallelize using MPI
2. Use proper MPI error handling
3. Learn how to use non-blocking communication methods
4. Use a debugger with parallelized code.

</font>


# 1. Demo 1.

## 1.1. Writing Hello World program
```c++
/* Filename: main.cpp */
#include <iostream>


int main(int argc, char **argv) {
    std::cout << "Hello world!\n";
    return 0;
}
```

Output:
```shell
$ mpicxx main.cpp -o main
$ mpirun -n 3 ./main
Hello world!
Hello world!
Hello world!
```

## 1.2. Getting Started with MPI -- Run this code `on multiple processes`.

```sh
$ mpiexec -n <number_of_processes> <command_to_launch_code>
```

For example, we run `./main` on 4 processes:
```sh
$ mpiexec -n 4 ./main
Hello World!
Hello World!
Hello World!
Hello World!
```

<font color="pink" size="4">

1. When you execute `mpiexec -n 4 ./main`, `mpiexec` launches 4 different instances of `./main` simultaneously, which each print `Hello World!`.

2. Typically, as long as you have at least 4 `processors` on the machine you are running on, `each process will be launched on a different processor`;<font color="gree">(MPI 启动的是进程)</font>
   1.  however, certain environment variables and optional arguments to mpiexec can change this behavior.
   2.  Each process runs the code in hello `independently of the others`.
3. The processes `mpiexec` launches aren't completely unaware of one another.
   1. The `mpiexec` adds each of the processes to `a MPI communicator`, which enables each of the processes to send and receive information to one another via `MPI`. 
   2. The `MPI communicator` that spans all of the processes launched by `mpiexec` is called `MPI_COMM_WORLD`

</font>

## 1.3. MPI Communicator -- `MPI_COMM_WORLD`
We can use the MPI library to get some information about the `MPI_COMM_WORLD` communicator and the processes within it. Edit main.cpp so that it reads as follows:

```c++
/* Filename: main.cpp */

#include <iostream>
#include "mpi.h"


int main(int argc, char **argv) {
    // Initialize MPI
    // This function must be called before any other MPI functions.
    MPI_Init(&argc, &argv);

    // Get the number of processes in MPI_COMM_WORLD
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    // Get the rank of this process in MPI_COMM_WORLD
    int my_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    
    // Print information about MPI_COMM_WORLD
    std::cout << "World Size: " << world_size << ", "
                << "Rank: " << my_rank << std::endl;
    
    // Finalize MPI 
    // This must always be called after all other MPI functions
    MPI_Finalize();

    return 0;
}
```

Output:
```sh
$ mpiexec -n 4 ./main
World Size: 4, Rank: 2
World Size: 4, Rank: 3
World Size: 4, Rank: 1
World Size: 4, Rank: 0
```

<font color="red" size="4">

Note
----
1. As you can see, the ranks doesn't print out their message in order.
2. If you run the code again, the ranks are likely to print thier message `in a different order.`

</font>


# 2. Error Handling with `MPI`
<font color="red" size="4">

Note
----
1. If an error forces an MPI program to exit, it `should never` just call `return` or `exit`. This is because calling `return` or `exit` might terminate one of the MPI processesm, but leave others running indefinely. 
2. If you're not carefully, you will waste massive amount of computational resource running a failed calculation you thought have been terminated.
3. `MPI-parallelized codes` should call `MPI_Abort()` when something goes wrong, as this function will ensure all MPI processes terminate.

</font>

## 2.1. `MPI_Abort(MPI_COMM_WORLD, int)`
<font color="pink" size="4">

The `MPI_Abort()` function takes two arguments: 
-----------------------------------------------
1. `MPI_COMM_WORLD`: The `MPI communicator` corresponding to the set of MPI processes to terminate `(this should generally be `MPI_COMM_WORLD`.)`
2. `int`: An `error code` that will be returned to the environment.

</font>

<font color="red" size="4">

Note
----
1. It is also useful to keep in mind that most `MPI functions` have a `return value` that indicates whether the function completed successfully. 
   1. If the `return_code == MPI_SUCCESS`, the function was executed succefully; otherwise, the function call failed.
2. By default, `MPI` functions automatically abort if they encounter an error, so you'll only get a return value of `MPI_SUCCESS`
3. If you want to handle a MPI errors yourself, you can call `MPI_Errhandler_set(MPI_COMM_WORLD, MPI_ERRORS_RETURN)`
   1. if you do this, you must check the return value of `every MPI function` and call `MPI_Abort`if it is not equal to `MPI_SUCCESS`

</font>

```c++
if (MPI_Init(&argc,&argv) != MPI_SUCCESS) 
    MPI_Abort(MPI_COMM_WORLD, 1);
```

## 2.2. Demo 
```c++
/* Numerical Integration Example that uses MPI error checking functions */


#include <mpi.h>
#include <math.h>
#include <stdio.h>
float fct(float x)
{
      return cos(x);
}
/* Prototype */
float integral(float a, int n, float h);
int main(int argc, char **argv)
{
/***********************************************************************
 *                                                                     *
 * This is one of the MPI versions of numerical integration            *
 * It demonstrates the use of :                                        *
 *                                                                     *
 * 1) MPI_Init                                                         *
 * 2) MPI_Comm_rank                                                    *
 * 3) MPI_Comm_size                                                    *
 * 4) MPI_Recv                                                         *
 * 5) MPI_Send                                                         *
 * 6) MPI_Finalize                                                     *
 * 7) MPI_Errhandler_set                                               *
 * 8) MPI_Error_class                                                  *
 * 9) MPI_Error_string                                                 *
 *                                                                     *
 ***********************************************************************/
      int n, p, i, j, ierr,num,errclass,resultlen;
      float h, result, a, b, pi;
      float my_a, my_range;
      char err_buffer[MPI_MAX_ERROR_STRING];

      int myid, source, dest, tag;
      MPI_Status status;
      float my_result;

      pi = acos(-1.0);  /* = 3.14159... */
      a = 0.;           /* lower limit of integration */
      b = pi*1./2.;     /* upper limit of integration */
      n = 100000;          /* number of increment within each process */

      dest = 10;         /* define the process that computes the final result */
      tag = 123;        /* set the tag to identify this particular job */

/* Starts MPI processes ... */

      MPI_Init(&argc,&argv);              /* starts MPI */
      MPI_Comm_rank(MPI_COMM_WORLD, &myid);  /* get current process id */
      MPI_Comm_size(MPI_COMM_WORLD, &p);     /* get number of processes */
      
      /*Install a new error handler */
      
      MPI_Errhandler_set(MPI_COMM_WORLD,MPI_ERRORS_RETURN); /* return info about
      errors */

      h = (b-a)/n;    /* length of increment */
      num = n/p;       /* number of intervals calculated by each process*/
      my_range = (b-a)/p;
      my_a = a + myid*my_range;
      my_result = integral(my_a,num,h);

      printf("Process %d has the partial result of %f\n", myid,my_result);

      if(myid == 0) {
        result = my_result;
        for (i=1;i<p;i++) {
          source = i;           /* MPI process number range is [0,p-1] */
          MPI_Recv(&my_result, 1, MPI_REAL, source, tag,
                        MPI_COMM_WORLD, &status);
          result += my_result;
        }
        printf("The result =%f\n",result);
      }
      else
        ierr= MPI_Send(&my_result, 1, MPI_REAL, dest, tag,
                      MPI_COMM_WORLD);      /* send my_result to intended dest. */
       if (ierr != MPI_SUCCESS) {
           MPI_Error_class(ierr,&errclass);
            if (errclass== MPI_ERR_RANK) {
            fprintf(stderr,"Invalid rank used in MPI send call\n");
           MPI_Error_string(ierr,err_buffer,&resultlen);
           fprintf(stderr,err_buffer);
           MPI_Finalize();             /* abort*/
           }
       }
       
      MPI_Finalize();                       /* let MPI finish up ... */
}
float integral(float a, int n, float h)
{
      int j;
      float h2, aij, integ;

      integ = 0.0;                 /* initialize integral */
      h2 = h/2.;
      for (j=0;j<n;j++) {             /* sum over all "j" integrals */
        aij = a + j*h;        /* lower limit of "j" integral */
        integ += fct(aij+h2)*h;
      }
      return (integ);
}
```