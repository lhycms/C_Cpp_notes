# 1. Overview of groups
<font color="green" size="4">

Part I
------
- While `MPI_Comm_split()` is the simplest way to create a new communicator, it isn’t the only way to do so. 
- There are more flexible ways to create communicators, but they use a new kind of MPI object, `MPI_Group`. 
- Internally, `MPI` has to keep up with (among other things) two major parts of a communicator:
    1. the `context` (or ID) that differentiates one communicator from another 
    2. the `group of processes` contained by the communicator. 
    3. The `context` is what prevents an operation on one communicator from matching with a similar operation on another communicator. MPI keeps an ID for each communicator internally to prevent the mixups. 
- The `group` is a little simpler to understand since it is `just the set of all processes in the communicator`. For `MPI_COMM_WORLD`, this is all of the processes that were started by mpiexec. For other communicators, the group will be different. In the example code above, the group is all of the processes which passed in the same color to `MPI_Comm_split`s.

Part II
-------
1. `MPI` uses these `groups` in the same way that `set` theory generally works. 
    - The `union operation (并集)` creates a new, (potentially) bigger set from two other sets. The new set includes all of the members of the first two sets (without duplicates). 
    - The `intersection operation (交集)` creates a new, (potentially) smaller set from two other sets. The new set includes all of the members that are present in both of the original sets. You can see examples of both of these operations graphically below.

</font>


# 2. Using `MPI groups`
## 2.1. `MPI_Comm_group()`: Get the group of processes in a communicator
```c++
MPI_Comm_group(
        MPI_Comm comm,
        MPI_Group *group)
```
<font color="green" size="4">

1. The `group object` works the same way as a `communicator object` except that you can’t use it to communicate with other ranks (because it doesn’t have that context attached). You can still get the rank and size for the group (`MPI_Group_rank` and `MPI_Group_size`, respectively). 
2. However, what you can do with groups that you can’t do with communicators is use it to `construct new groups locally`. 

Difference between a `local operation` and a `remote` one:
--- 
1. A remote operation involves communication with other ranks where a local operation does not. `Creating a new communicator is a remote operation` because all processes need to decide on the same context and group
2. `creating a group is local` because it isn’t used for communication and therefore doesn’t need to have the same context for each process. 
3. You can manipulate a group all you like without performing any communication at all.

</font>


## 2.2. `MPI_Group_union()`, `MPI_Group_intersection()`
<font color="orange" size="4">

- In both cases, the operation is performed on group1 and group2 and the result is stored in newgroup.

</font>

```c++
MPI_Group_union(
	MPI_Group group1,
	MPI_Group group2,
	MPI_Group* newgroup)

MPI_Group_intersection(
	MPI_Group group1,
	MPI_Group group2,
	MPI_Group* newgroup)
```


## 2.3. `MPI_Comm_create_group()`
<font color="green" size="4">

There are many uses of groups in `MPI`. You can compare groups to see if they are the same, subtract one group from another, exclude specific ranks from a group, or use a group to translate the ranks of one group to another group. However, one of the recent additions to MPI that tends to be most useful is `MPI_Comm_create_group`. `This is a function to create a new communicator, but instead of doing calculations on the fly to decide the makeup`, like `MPI_Comm_split`, this function takes an `MPI_Group` object and creates a new communicator that has all of the same processes as the group.

</font>

```c++
MPI_Comm_create_group(
	MPI_Comm comm,
	MPI_Group group,
	int tag,
	MPI_Comm* newcomm)
```


# 3. Example of using groups
<font color="green" size="4">


Let’s look at a quick example of what using groups looks like. Here, we’ll use another new function which allows you to pick specific ranks in a group and construct a new group containing only those ranks, `MPI_Group_incl`.
```c++
MPI_Group_incl(
	MPI_Group group,
	int n,
	const int ranks[],
	MPI_Group* newgroup)

/*
`newgroup` contains the processes in `group` with ranks contained in `ranks`, which is of size `n`. 
*/
```

</font>

```c++
#include <iostream>
#include "mpi.h"


int main(int argc, char **argv) {
    MPI_Init(&argc, &argv);
    // Get the rank and size in the original communicator
    int world_rank, world_size;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    // Get the group of process in MPI_COMM_WORLD
    MPI_Group world_group;
    MPI_Comm_group(MPI_COMM_WORLD, &world_group);

    int n=7;
    const int ranks[7] = {1, 2, 3, 5, 7, 11, 13};

    // Construct a group containing all of the prime ranks in world_group
    MPI_Group prime_group;
    MPI_Group_incl(world_group, 7, ranks, &prime_group);

    // Create a new communicator based on the group
    MPI_Comm prime_comm;
    MPI_Comm_create_group(MPI_COMM_WORLD, prime_group, 0, &prime_comm);

    int prime_rank = -1, prime_size = -1;
    // If this rank isn't in the new communicator, it will be
    // MPI_COMM_NULL. Using MPI_COMM_NULL for MPI_Comm_rank or
    // MPI_Comm_size is erroneous
    if (MPI_COMM_NULL != prime_comm) {
        MPI_Comm_rank(prime_comm, &prime_rank);
        MPI_Comm_size(prime_comm, &prime_size);
    }

    printf("WORLD RANK/SIZE: %d/%d \t PRIME RANK/SIZE: %d/%d\n",
        world_rank, world_size, prime_rank, prime_size);

    MPI_Group_free(&world_group);
    MPI_Group_free(&prime_group);
    MPI_Comm_free(&prime_comm);

    MPI_Finalize();

    return 0;
}
```